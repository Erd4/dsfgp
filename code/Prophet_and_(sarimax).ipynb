{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# -*- coding: utf-8 -*-\n", "\"\"\"\n", "Created on Sun Dec  4 20:44:21 2022"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@author: iphon\n", "\"\"\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import prophet as Prophet"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Importing modules for Prophet"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[8]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"data_gp/total_df.csv\").drop(columns=[\"Unnamed: 0\"])\n", "df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reparing df for prophet to work with"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["kol = df.columns.to_list()\n", "add_regs_list=[]\n", "for i in range(230):\n", "    add_regs_list.append(\"add\"+str(i+1))\n", "add_regs_list\n", "rename_df = dict(zip(kol,add_regs_list))\n", "rename_df['DATE'] = \"ds\"\n", "rename_df['Herkunftsland - Total'] = \"y\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["renaming DF with new column names:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.rename(columns={\"DATE\":\"ds\",\"Herkunftsland - Total\":\"y\"},inplace=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["inally dropping all the country visitors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["listen = ['Argentina',\n", " 'Australien',\n", " 'Austria',\n", " 'Bahrain',\n", " 'Belarus',\n", " 'Belgium',\n", " 'Brasil',\n", " 'Bulgaria',\n", " 'Canada',\n", " 'China',\n", " 'Croatia',\n", " 'Cyprus',\n", " 'Czech Republic',\n", " 'Denmark',\n", " 'Egypt',\n", " 'Estonia',\n", " 'Finland',\n", " 'France',\n", " 'Germany',\n", " 'Greece',\n", " 'Hongkong',\n", " 'Hungary',\n", " 'Iceland',\n", " 'India',\n", " 'Indonesia',\n", " 'Ireland',\n", " 'Israel',\n", " 'Italy',\n", " 'Japan',\n", " 'Kuwait',\n", " 'Latvia',\n", " 'Liechtenstein',\n", " 'Lithuania',\n", " 'Luxembourg',\n", " 'Malaysia',\n", " 'Malta',\n", " 'Mexico',\n", " 'Netherlands',\n", " 'New Zealand',\n", " 'Norway',\n", " 'Oman',\n", " 'Philippinen',\n", " 'Poland',\n", " 'Portugal',\n", " 'Qatar',\n", " 'Romania',\n", " 'Russia',\n", " 'Saudi Arabia',\n", " 'Serbia',\n", " 'Singapore',\n", " 'Slovakia',\n", " 'Slovenia',\n", " 'South Africa',\n", " 'South Korea',\n", " 'Spain',\n", " 'Sweden',\n", " 'Switzerland',\n", " 'Taiwan',\n", " 'Thailand',\n", " 'Turkey',\n", " 'Ukraine',\n", " 'United Arab Emirates',\n", " 'United Kingdom',\n", " 'United States']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.drop(columns=listen,inplace=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we're going to simulate the first Prophet prediction without adding regressors"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[10]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fitting dataframe into Prophet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["m = Prophet.Prophet(yearly_seasonality=True)\n", "m.fit(df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["aking a new DF for our prediction for the future<br>\n", "uture = m.make_future_dataframe(periods=6, freq=\"MS\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["future = m.make_future_dataframe(periods=6, freq = 'MS')\n", "future.tail()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["etting prophet to forecast future guest numbers into the new future-df based on the data in our original df<br>\n", "cst = m.predict(future)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["forecast_m = m.predict(future)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cross validating data and getting performance metrics"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[11]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from prophet.diagnostics import cross_validation\n", "df_cv_m = cross_validation(m, horizon = '1y')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Getting performance metrics"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[12]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from prophet.diagnostics import performance_metrics\n", "df_p_m = performance_metrics(df_cv_m)\n", "m_mean_mae = np.mean(df_p_m['mae'])\n", "m_mean_rmse = np.mean(df_p_m['rmse'])\n", "m_mean_smape = np.mean(df_p_m['smape'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plotting results"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[13]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from prophet.plot import plot_cross_validation_metric\n", "ax1_m = m.plot(forecast_m, include_legend= True, xlabel = 'Year', ylabel = 'Visitors')\n", "ax2_m = m.plot_components(forecast_m)\n", "ax3_m = plot_cross_validation_metric(df_cv_m, metric='mae')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now trying to fit some exogenous variables and be a bit more precise so lets check the correlation first:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[14]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.corrwith(df[\"y\"]).nsmallest(6)\n", "df.corrwith(df[\"y\"]).nlargest(9)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As we can see we have some very high correlations like st.moritz visitors but also low ones like rainy days with not that much downfall,<br>\n", "thus we are going to add our top 5 correlated regressors now that are not other communes."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[15]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["m2 = Prophet.Prophet(yearly_seasonality=True)\n", "m2.add_regressor(\"cm avg. snowheight - chd\", standardize=False)\n", "m2.add_regressor('cm avg. snowheight - gsg', standardize=False)\n", "m2.add_regressor('cm neuschnee - chd', standardize=False)\n", "m2.add_regressor('eistage - chd', standardize=False)\n", "m2.add_regressor('frosttage - chd', standardize=False)\n", "m2.fit(df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Creating future dataframes for all the different regressors"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[16]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["m_snow_chd = Prophet.Prophet(yearly_seasonality=True)\n", "df_snow_chd = df.rename(columns = {'y' : 'bad', 'cm avg. snowheight - chd' : 'y'})\n", "m_snow_chd.fit(df_snow_chd)\n", "future_df_snow_chd = m_snow_chd.make_future_dataframe(periods=36, freq = 'MS')\n", "forecast_snow_chd = m_snow_chd.predict(future_df_snow_chd)\n", "forecast_snow_chd.rename(columns = {'yhat' : 'cm avg. snowheight - chd'}, inplace = True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["m_snow_gsg = Prophet.Prophet(yearly_seasonality=True)\n", "df_snow_gsg = df.rename(columns = {'y' : 'bad', 'cm avg. snowheight - gsg' : 'y'})\n", "m_snow_gsg.fit(df_snow_gsg)\n", "future_df_snow_gsg = m_snow_gsg.make_future_dataframe(periods=36, freq = 'MS')\n", "forecast_snow_gsg = m_snow_gsg.predict(future_df_snow_gsg)\n", "forecast_snow_gsg.rename(columns = {'yhat' : 'cm avg. snowheight - gsg'}, inplace = True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["m_new_snow = Prophet.Prophet(yearly_seasonality=True)\n", "df_new_snow = df.rename(columns = {'y' : 'bad', 'cm neuschnee - chd' : 'y'})\n", "m_new_snow.fit(df_new_snow)\n", "future_df_new_snow = m_new_snow.make_future_dataframe(periods = 36, freq = 'MS')\n", "forecast_new_snow = m_new_snow.predict(future_df_new_snow)\n", "forecast_new_snow.rename(columns = {'yhat' : 'cm neuschnee - chd'}, inplace = True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["m_ice = Prophet.Prophet(yearly_seasonality=True)\n", "df_ice = df.rename(columns = {'y' : 'bad', 'eistage - chd' : 'y'})\n", "m_ice.fit(df_ice)\n", "future_df_ice = m_ice.make_future_dataframe(periods=36, freq = 'MS')\n", "forecast_ice = m_ice.predict(future_df_ice)\n", "forecast_ice.rename(columns = {'yhat' : 'eistage - chd'}, inplace = True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["m_frost = Prophet.Prophet(yearly_seasonality=True)\n", "df_frost = df.rename(columns = {'y' : 'bad', 'frosttage - chd' : 'y'})\n", "m_frost.fit(df_frost)\n", "future_df_frost = m_frost.make_future_dataframe(periods=36, freq = 'MS')\n", "forecast_frost = m_frost.predict(future_df_frost)\n", "forecast_frost.rename(columns = {'yhat' : 'frosttage - chd'}, inplace = True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Creating future dataframe for our main dataframe and merging it with the different regressors."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[17]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["future_df = m2.make_future_dataframe(periods=36, freq = 'MS')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["future_df = pd.merge(future_df, forecast_frost[['frosttage - chd', 'ds']], on = 'ds', how = 'inner')\n", "future_df = pd.merge(future_df, forecast_ice[['eistage - chd', 'ds']], on = 'ds', how = 'inner')\n", "future_df = pd.merge(future_df, forecast_new_snow[['cm neuschnee - chd', 'ds']], on = 'ds', how = 'inner')\n", "future_df = pd.merge(future_df, forecast_snow_gsg[['cm avg. snowheight - gsg', 'ds']], on = 'ds', how = 'inner')\n", "future_df = pd.merge(future_df, forecast_snow_chd[['cm avg. snowheight - chd', 'ds']], on = 'ds', how = 'inner')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Forecasting our dataframe with our different regressors."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[18]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["forecast_m2 = m2.predict(future_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cross validating"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[19]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_cv_m2 = cross_validation(m2, horizon = '1y')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Getting our different metrics to compare old prediction with new prediction, namely functions m and m2."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[20]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_p_m2 = performance_metrics(df_cv_m2)\n", "m2_mean_mae = np.mean(df_p_m2['mae'])\n", "m2_mean_rmse = np.mean(df_p_m2['rmse'])\n", "m2_mean_smape = np.mean(df_p_m2['smape'])\n", "# Plotting the new prediction"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[21]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ax1_m2 = m2.plot(forecast_m2, include_legend= True, xlabel = 'Year', ylabel = 'Visitors')\n", "ax2_m2 = m2.plot_components(forecast_m2)\n", "ax3_m2 = plot_cross_validation_metric(df_cv_m2, metric='mae')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[12]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "fig, axs = plt.subplots(nrows = 3, ncols = 2, figsize = (12,6), dpi = 1000)\n", "sns.lineplot(data = df_p_m2['rmse'], ax = axs[0,0]).axhline(np.mean(df_p_m2['rmse']), c='red', linestyle='dashed', label=\"Mean\")\n", "sns.lineplot(data = df_p_m2['mae'], ax = axs[1,0]).axhline(np.mean(df_p_m2['mae']), c='red', linestyle='dashed')\n", "sns.lineplot(data = df_p_m2['smape'], ax = axs[2,0]).axhline(np.mean(df_p_m2['smape']), c='red', linestyle='dashed')\n", "sns.lineplot(data = df_p_m['rmse'], ax = axs[0,1]).axhline(np.mean(df_p_m['rmse']), c='red', linestyle='dashed')\n", "sns.lineplot(data = df_p_m['mae'], ax = axs[1,1]).axhline(np.mean(df_p_m['mae']), c='red', linestyle='dashed')\n", "sns.lineplot(data = df_p_m['smape'], ax = axs[2,1]).axhline(np.mean(df_p_m['smape']), c='red', linestyle='dashed')\n", "axs[0,0].set_title('Prediction with Regressors')\n", "axs[0,1].set_title('Prediction without Regressors')\n", "axs[0,0].set_ylim(2400, 7700)\n", "axs[0,1].set_ylim(2400, 7700)\n", "axs[1,0].set_ylim(1700, 5500)\n", "axs[1,1].set_ylim(1700, 5500)\n", "axs[2,0].set_ylim(0.05, 0.47)\n", "axs[2,1].set_ylim(0.05, 0.47)\n", "fig.legend(loc = \"lower center\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[30]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_bar = pd.DataFrame(index = [0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_bar['m2_mean_rmse'] = m2_mean_rmse\n", "df_bar['m_mean_rmse'] = m_mean_rmse\n", "df_bar['m2_mean_mae'] = m2_mean_mae\n", "df_bar['m_mean_mae'] = m_mean_mae\n", "df_bar['m2_mean_smape'] = m2_mean_smape\n", "df_bar['m_mean_smape'] = m_mean_smape"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax_bar = plt.subplots(1,3,figsize = (12, 6), dpi = 1000)\n", "palette = sns.color_palette(\"Paired\")\n", "sns.barplot(data = df_bar[['m2_mean_rmse','m_mean_rmse']], ax = ax_bar[0], palette = palette).set(xticklabels=['With Regressors', 'Without Regressors']) \n", "sns.barplot(data = df_bar[['m2_mean_mae', 'm_mean_mae']], ax = ax_bar[1], palette = palette).set(xticklabels=['With Regressors', 'Without Regressors']) \n", "sns.barplot(data = df_bar[['m2_mean_smape', 'm_mean_smape']], ax = ax_bar[2], palette = palette).set(xticklabels=['With Regressors', 'Without Regressors']) \n", "ax_bar[0].set_title('Mean of Root-Mean-Squared Error')\n", "ax_bar[1].set_title('Mean of Mean Absolute Error')\n", "ax_bar[2].set_title('Mean of Symmetric Mean Absolute Percentage Error')\n", "fig.suptitle('Various Metrics for Prophet Predictions with and without regressors')\n", "# Using a second model, namely SARIMAX, to predict the visitors in the region"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[9]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from datetime import timedelta\n", "from datetime import datetime\n", "from time import time\n", "from statsmodels.tsa.statespace.sarimax import SARIMAX\n", "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n", "import pandas as pd\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "RANDOM_SEED = np.random.seed(0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(\"data_gp/total_df.csv\").drop(columns=[\"Unnamed: 0\"])\n", "df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])\n", "df = df.set_index(\"DATE\")\n", "df[\"Herkunftsland - Total\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[17]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["anually splitting our DF into different seasons IRRELEVANT BY NOW BC WE USE SARIMAX<br>\n", ",f,s,h = ph.split_seasons(df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[4]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ist with all the countries so we can drop them for our preds"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["listen = ['Argentina',\n", " 'Australien',\n", " 'Austria',\n", " 'Bahrain',\n", " 'Belarus',\n", " 'Belgium',\n", " 'Brasil',\n", " 'Bulgaria',\n", " 'Canada',\n", " 'China',\n", " 'Croatia',\n", " 'Cyprus',\n", " 'Czech Republic',\n", " 'Denmark',\n", " 'Egypt',\n", " 'Estonia',\n", " 'Finland',\n", " 'France',\n", " 'Germany',\n", " 'Greece',\n", " 'Hongkong',\n", " 'Hungary',\n", " 'Iceland',\n", " 'India',\n", " 'Indonesia',\n", " 'Ireland',\n", " 'Israel',\n", " 'Italy',\n", " 'Japan',\n", " 'Kuwait',\n", " 'Latvia',\n", " 'Liechtenstein',\n", " 'Lithuania',\n", " 'Luxembourg',\n", " 'Malaysia',\n", " 'Malta',\n", " 'Mexico',\n", " 'Netherlands',\n", " 'New Zealand',\n", " 'Norway',\n", " 'Oman',\n", " 'Philippinen',\n", " 'Poland',\n", " 'Portugal',\n", " 'Qatar',\n", " 'Romania',\n", " 'Russia',\n", " 'Saudi Arabia',\n", " 'Serbia',\n", " 'Singapore',\n", " 'Slovakia',\n", " 'Slovenia',\n", " 'South Africa',\n", " 'South Korea',\n", " 'Spain',\n", " 'Sweden',\n", " 'Switzerland',\n", " 'Taiwan',\n", " 'Thailand',\n", " 'Turkey',\n", " 'Ukraine',\n", " 'United Arab Emirates',\n", " 'United Kingdom',\n", " 'United States',\n", " \"Herkunftsland - Total\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "#preparing our df and defining exogenous variables as well as our endogenous variable\n", "#creating array with n observations of variables and k number of variables and then filling it\n", "exog = df.drop(columns=listen).to_numpy()\n", "exo = df.drop(columns=listen)\n", "endo = df[\"Herkunftsland - Total\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["lotting endo to check if it's static, which it seems..."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.lineplot(endo)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ut to be really sure we can check it's stationarity with an augmented dickey-fuller-test"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ph.check_stationarity(endo)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[6]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["exog"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Freude herrscht! Apparently it's not - as if it wasn't hard enough already<br>\n", "*at least we now know that d is not equal to 0 - but i'll get to that later*"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[7]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["et's try to make endo stationary by removing the trend:<br>\n", "we try to do this by taking the difference of between the current value and the prior month's value"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["endo_diff = endo.diff()[1:] \n", "exog_diff = np.diff(exog)[1:] \n", "# let's check\n", "sns.lineplot(endo_diff)\n", "plt.show()\n", "#and also Dickey-Fuller agrees with us:\n", "ph.check_stationarity(endo_diff)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[8]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ow we are using STL decomposition for our DF<br>\n", "ecomposing DF into Trend, Seasonality and Residue so we can fit it into our seasonal ARIMA(X)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from statsmodels.tsa.seasonal import seasonal_decompose"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_seasonal = seasonal_decompose(endo, model='additive')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["t's important to see a strong seasonal influence - otherwise we use ARIMA(X)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.rc('figure',figsize=(14,8))\n", "plt.rc('font',size=15)\n", "df_seasonal.plot();\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Here we can clearly see that there is some kind of trend (*and a remarkable one to say the least*) in our df<br>\n", "#### we also notice the big seasonal differences"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[9]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Trying to evaluate the ideal values of p and q using partial autocorrelation / autocorrelation functions<br>\n", "See --> https://otexts.com/fpp2/non-seasonal-arima.html#acf-and-pacf-plots"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_acf(endo_diff, lags=20);9\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **The biggest spike is at the 12 months mark, thus implies we should continue our work with a p value of 12**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[10]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_pacf(endo_diff, lags=20);\n", "plt.ylim(-2.5,5)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### There are **VERY** significant spikes at12 in our AFC and PAFC plot. - the huge spike at the 16 month could be covid -idk tho<br>\n", "<br>\n", "# Let's get training and testing sets now. so we can make sure our model works correctly"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[11]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ross Validation by using sklearns TSSplit"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import TimeSeriesSplit"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tss = TimeSeriesSplit(n_splits=6)\n", "#split our TS in test and train sets\n", "for train_index, test_index in tss.split(exog):\n", "    exog_train, exog_test = exog[train_index, :], exog[test_index,:]\n", "    endo_train, endo_test = endo.iloc[train_index], endo.iloc[test_index]\n", "    exo_train, exo_test = exo.iloc[train_index], exo.iloc[test_index]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[12]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["howing train and test set in one plot to get an understanding of the split"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["endo_train.groupby('DATE').mean().plot()\n", "endo_test.groupby('DATE').mean().plot()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[13]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["et's now define our training endos and exos with a d=1 and p=12 // order = pdq is 0,1,0 bc the date is very heavily influenced by seasonality however we were able to appply diff1 to it to integrate it"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["smodel = SARIMAX(endog=endo_train,exog=exo_train, order=(1,0,0), seasonal_order=(0,0,2,12))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[14]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["et's fit the model now:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["start = time()\n", "fit_smodel = smodel.fit()\n", "end = time()\n", "print(\"The model took \", end - start,\"seconds to be fitted\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["UTPUT:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["RUNNING THE L-BFGS-B CODE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["           * * *"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Machine precision = 2.220D-16<br>\n", " N =          167     M =           10"]}, {"cell_type": "markdown", "metadata": {}, "source": ["At X0         0 variables are exactly at the bounds"]}, {"cell_type": "markdown", "metadata": {}, "source": ["At iterate    0    f= -1.03835D+01    |proj g|=  1.05141D+14<br>\n", " This problem is unconstrained."]}, {"cell_type": "markdown", "metadata": {}, "source": [" Line search cannot locate an adequate point after MAXLS<br>\n", "  function and gradient evaluations.<br>\n", "  Previous x, f and g restored.<br>\n", " Possible causes: 1 error in function or gradient evaluation;<br>\n", "                  2 rounding error dominate computation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["           * * *"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Tit   = total number of iterations<br>\n", "Tnf   = total number of function evaluations<br>\n", "Tnint = total number of segments explored during Cauchy searches<br>\n", "Skip  = number of BFGS updates skipped<br>\n", "Nact  = number of active bounds at final generalized Cauchy point<br>\n", "Projg = norm of the final projected gradient<br>\n", "F     = final function value"]}, {"cell_type": "markdown", "metadata": {}, "source": ["           * * *"]}, {"cell_type": "markdown", "metadata": {}, "source": ["   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F<br>\n", "  167      1     21      1     0     0   1.051D+14  -1.038D+01<br>\n", "  F =  -10.383511680414193     "]}, {"cell_type": "markdown", "metadata": {}, "source": ["ABNORMAL_TERMINATION_IN_LNSRCH                              <br>\n", "The model took  11.60127305984497 seconds to be fitted"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[15]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["forecast = fit_smodel.forecast(len(endo_test), exog=exo_test)\n", "forecast = pd.Series(forecast, index=endo_test.index)\n", "residue = endo_test - forecast"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16,4))\n", "plt.plot(residue, label=\"Residual\")\n", "plt.axhline(0, linestyle=\"--\")\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"our MAE is\", np.mean(np.abs(endo_test - forecast)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[16]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ow we can plot our predictions to see whether we were doing alright"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16,4))\n", "plt.plot(forecast, label=\"Pred\")\n", "plt.plot(endo, label=\"True\")\n", "#plt.plot(predictions4, label=\"Predicted3\")\n", "plt.title(\"Number of guests in the tourism region of Gstaad\", fontsize=20)\n", "plt.ylabel('Visitors', fontsize=16) \n", "plt.ylim()\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[219]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[17]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_end = datetime(2021,5,1)\n", "test_end = datetime(2022,9,1)\n", "startparams = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1,1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["traintrain_exo = exo[train_end:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rolling_predictions = endo_test.copy()\n", "for train_end in endo_test.index:\n", "    train_data = endo[:train_end-timedelta(days=1)]\n", "    train_exo = exo[:train_end-timedelta(days=1)]\n", "    model = SARIMAX(train_data, order=(1,2,0), seasonal_order=(0,2,1,12),exog=train_exo, )\n", "    model_fit = model.fit(start_params=None)\n", "    \n", "    pred = model_fit.forecast(exog=traintrain_exo.loc[[train_end-timedelta()]])\n", "    rolling_predictions[train_end] = pred"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[18]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rolling_residuals = endo_test - rolling_predictions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[19]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10,4))\n", "plt.plot(rolling_residuals)\n", "plt.axhline(0, linestyle='--', color='k')\n", "plt.title('Rolling Forecast Residuals from SARIMA Model', fontsize=20)\n", "plt.ylabel('Error', fontsize=16)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[20]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(16,4))\n", "plt.plot(rolling_predictions, label=\"Pred\")\n", "plt.plot(endo, label=\"True\")\n", "#plt.plot(predictions4, label=\"Predicted3\")\n", "plt.title(\"Number of guests in the tourism region of Gstaad\", fontsize=20)\n", "plt.ylabel('Visitors', fontsize=16) \n", "plt.ylim()\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def smape(a, f):\n", "    return 1/len(a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["smape(endo_test, rolling_predictions)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from statsmodels.tools.eval_measures import rmse\n", "from sklearn.metrics import mean_absolute_error"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rmse(endo_test, rolling_predictions)\n", "mean_absolute_error(endo_test, rolling_predictions)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_bar_max = pd.DataFrame(index = [0])\n", "df_bar_max['Mean Absolute Error'] = mean_absolute_error(endo_test, rolling_predictions)\n", "df_bar_max['Root-Mean-Squared Deviation'] = rmse(endo_test, rolling_predictions)\n", "df_bar_max['Symmetric Mean Absolute Percentage Error'] = smape(endo_test, rolling_predictions)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axss = plt.subplots(1,3, figsize = (12,6), dpi = 1000, constrained_layout=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_bar_max['m_mean_rmse'] = df_bar['m_mean_rmse']\n", "df_bar_max['m_mean_mae']  = df_bar['m_mean_mae']\n", "df_bar_max['m_mean_smape'] = df_bar['m_mean_smape']\n", "sns.barplot(data = df_bar_max[['Root-Mean-Squared Deviation', 'm_mean_rmse']], ax = axss[0], palette = palette).set(xticklabels = ['SARIMAX', 'Prophet without regressors'],ylabel = 'Root-Mean-Squared Deviation')\n", "sns.barplot(data = df_bar_max[['Mean Absolute Error', 'm_mean_mae']], ax = axss[1], palette = palette).set(xticklabels = ['SARIMAX', 'Prophet without regressors'],ylabel = 'Mean Absolute Error')\n", "sns.barplot(data = df_bar_max[['Symmetric Mean Absolute Percentage Error','m_mean_smape']], ax = axss[2], palette = palette).set(xticklabels = ['SARIMAX', 'Prophet without regressors'],ylabel = 'Symmetric Mean Absolute Percentage Error')\n", "fig.suptitle('Comparison of Different Metrics between SARIMAX and Prophet Predictions')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}